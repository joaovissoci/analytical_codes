# AUTOMAP-SUS: Estrutura de coleta, gerenciamento e monitoramento geoespacial de grandes bancos de dados em saúde pública.

Within this proposal we intend to build a Big Clinical Data structure to be used for multiple applications in Uganda. This structure will be used to manage and deploy advanced data analysis techniques facilitating the generation of relevant clinical information from raw high quality data in automated and adaptive web applications. The elements of such innovative environment are described below.

![] Figure for the process

### Settings and population

### Virtual Cloud Reposistory. 

The role of a cloud repository is to store all the data so that it can accessed by researchers during their projects. All anonymized data will be stored under a cloud environment hosted within Duke University under infrastructure provided by [...]. When working on international collaboration, we will anonymize all data so that individual patients cannot be identified (more details in the following section about data de-identification) and store the data under the Amazon Elastic Compute Cloud (Amazon EC2) using a Linux Micro Instance. This repository will contain a MySQL database server where the database is available in a normalized format. This database will be directly updated through a data collection system (REDCAp), which updates the system in real time. The database will be connected to a D2RQ Platform (version 0.8.1), which converts the relational structure into a RDF (Resource Description Framework) format and an accompanying SPARQL (a recursive acronym for SPARQL Protocol and RDF Query Language, http://www.w3.org/TR/rdf-sparql-query/) endpoint. This endpoint allows for direct data consumption through queries and connection with other data sets from the Linked Open Data (LOD) cloud (http://linkeddata.org/).

The cloud will also contain an instance of the R statistical language (version 3.2.2), so that users can easily manipulate statistical scripts, generate reports, and directly upload them to the server on the same cloud environment. Finally, the whole system is remotely backed up on a daily basis, thus minimizing the risk of any loss and also accelerating its recovery in case of an accident. Of importance,  no data will be published without a signed authorization by the Principal Investigator.

### Data collection and management

#### Data collection tools

#### Data de-identification. 

The importance of data de-identification is to protect the privacy of patients, and this will be treated with the utmost priority. All data within our cloud will be de-identified prior to its transference to our server. The de-identification is conducted by transforming all dates into intervals from a randomly selected date that is securely stored by a database manager. ZIP codes are treated in way so as to comply with privacy regulations from the US. For the storage within the EC2 cloud, ZIP codes are stored with three digits, therefore complying with HIPAA (Health Insurance Portability and Accountability Act) regulations. All other elements constituting Protected Health Information (PHI http://www.hipaa.com/2009/09/hipaa-protected-health-information-what-does-phi-include/) are stripped from the final file so that the protection of patient privacy is ensured.

#### Data standardization. 

The importance of data standardization is to ensure that our prospective registry databases are codified in the same way so that comparisons and data merging can occur for specific projects. In order to ensure that the databases included in our data storage system will be compatible with other international data sets, we standardized all variable to be compliant with the [... Ben's pediatric data]. Special emphasis will be placed on socio-demographic description and lists of comorbidities since they are commonly used for risk adjustment, geolocation variables as they are a common method for linkage to large Linked Open Data (LOD, http://linkeddata.org/) repositories. Although standardization could be established in languages other than English, we opted to keep all variables standardized only in English as at this point this is widely recognized as the “scientific and business language.”

### Resources for automated data quality control, geographical surveillance and risk prediction calculators. 

The importance of automated data quality control, association and prediction reports is to make sure that researchers can quickly understand the contents of each database, ultimately deciding whether they will be adequate to answer their research questions. In order to provide end users with an assurance regarding data quality of the primary and subsequent data sets added to the open data collection project, we will establish a system to generate automated data quality control reports. This system is based on the statistical language R, specifically with a set of packages that uses literate programming to automatically transform the statistical results into a real-time interactive web applications and PDF (Portable Document Format), deployed to our central Web site. The literate programming will be generated using the Shiny R application package (R SHINY site). Scripts for all of our procedures will be available within our Github repository at [...]

#### System architecture

Advanced data analysis resources will be used to provide the infrastructure that will allow clinical and quantitative researchers to answer their research questions. Making use of parallel computing ans hosted on a Amazon Elastic Compute Cloud (Amazon EC2), modeling resources will include the R statistical language translating advances research methods to directly and automatedly conduct analyses within our server, to generate interactive applications for clinicians to get knowledge out of data in real-time. We intend to add features such as risk prediction calculators based on machine learning techniques and complex variable systems interactions and simulation through bayesian networks. 

The web applications will be developed using the Shiny R package, which facilitates building interactive web applications [“Introducing Shiny: Easy web applications in R | RStudio Blog.” [Online]. Available:
http://blog.rstudio.org/2012/11/08/introducing-shiny/.]. It provides the possibility to load the datasets for the analysis and to access all functions of other supplied R statistical functions. The Shiny application features two basic interfaces: a user-interface definition and a server script file(Fig X). Both of these components are controlled by the code that is written within the frame work of Shiny application in R. The building block of Shiny package is based on reactive programming [ref 15 Z. Wan and P. Hudak, “Functional reactive programming from first principles,” ACM SIGPLAN Not., vol. 35, no. 5, pp. 242–252, May 2000.]. Since the major task of the web application is to get the inputs and produce outputs, the whole programming language is designed so that a change in any input whether it is input data or method parameters will change the end result. The practical application of this concept is, for instance, based on a predictive model one can add new input for an individual patient and get individual redicted risks. The change will immediately be reflected into the form of texts, tables or figures. Within the shiny package, ordinary controllers or widgets are provided for ease of use of application programmers.


![] Figure for the system architecture

Along with the project's Github repository, we will include a series of training modules, also available within our github repository (...), along with video screencasts providing details on how to execute and interpret each analysis, as well as using our web application. 

#### Predictive Modeling. 

Predictive modeling techniques are used to assess, monitor and control environmental factors of a given taxon. Such approach is one an essential steps in the development of risk classification for health related outcomes based on a set of clinical, sociodemographical and environmental variables. The goal of applying different predictive models is to simplify complex systems and to enable reliable predictions. Machine learning is a relatively new science field focused on the construction and study of predictive systems [refs]. The main goal of the machine learning field is to construct suitable models from a set of predictive variables that can perform certain tasks [2]. Among the techniques of machine learning, the classification and regression tree (CART) algorithms are of special interest to health sciences due to its flexibility and interpretability (ref). It is useful to discover which variable or combination of variables better predicts a given outcome (ill or not-ill, for example) and to identify which cutoff points for each variable generates the most accurate prediction. CART models are flexible to deal with any kind of variable (nominal, ordinal, interval and ratio) at the same time, and can be applied in a wide range of problems [3]. Finally, the algorithm verifies which variable, or their combinations, better predicts the outcome and also identifies the cut-off points that are maximally predictive. To deal with overfitting and generalization we will apply techniques that aggregate different predictive models into a single model, also called the “ensemble” strategy [1, 2] such as the random forest. All analysis will be conducted through R Language, with the support of the caret package (ref) and transformed into a risk classification calculators as deliverable and innovative for clinicians in Uganda.

#### Bayesian Modeling. 

After defining the best predicting variables and as a form of external validity, we will model the causal structure of the data applying Bayesian Networks (BN) algorithms. BN are Directed Acyclic Graphs that allows us to explore the relationships between the variables. In this graph analysis each node represents a variable and each edges represent the conditional dependency relationship between two nodes (variables), which can be calculated by probability distributions or even logistic regression (Nielsen & Jensen, 2007). To build the model, we used a score-based algorithm for gaussian data called Hill Climbing (hc) available within the “bnlearn” package (Scutari, 2015) for R Language (ref). This approach executes a greedy search technique (take decisions based on the best available information, not worrying about the future effects of such decisions) on the space of the directed graphs (Margaritis, 2003). During the execution of score-based algorithms, each node receives a score (network score), which will reflect the quality of fit. Then the algorithm tries to maximize this score in a iterative way in order to achieve the structure whose score is higher (Scutari, 2015). As part of this process, the algorithm need to learn the Markov Blankets for each variable which consists in discovering of all dependency relationships that best fit the model (parent and children variables). In summary, we consider all the variables that can provide information about the variable that represents the node in question (Tsamardinos, 2003). The validation of the quality of BN will be performed by bootstrap resampling (Sachs et al. 2005), learning 500 networks structures and evaluating arc that appears in at least 85% of all networks generated and direction that appears in 50% of the time in each direction. The upside of applying BN as a predictive tool is that based on its bayesian characteristics, we want to allow clinicians to interact with the modeling process through a web application and set different proportion priors as a way to simulate changes in the network and its consequent impact on the overall model and health outcomes. Putting this together as a deliverable tool for clinicians, we anticipate that this modeling technique might be used as a management tool to develop evidence-based interventions to improve health outcomes within the hospital setting.

#### Geographical Information Systems
